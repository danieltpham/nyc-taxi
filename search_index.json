[
["index.html", "Exploring Yellow Taxi Profitability in NYC: A Spatio-Temporal Analysis Preface", " Exploring Yellow Taxi Profitability in NYC: A Spatio-Temporal Analysis Daniel Pham October 2020 Preface Disclaimer This work was submitted as a coursework assignment for MAST30034 - Applied Data Science at The University of Melbourne (October 2020), and is published as a portfolio item of the author. Please kindly do not misuse this work for any other purposes. "],
["1-phase1.html", "Phase 1 Data Visualisation", " Phase 1 Data Visualisation Ever since the Uber’s inception into the daily lives of New Yorkers, the booming of share-riding services has added another pressure on the deteriorating taxi industry that is characterized by low profitability and high running costs 1. While the Limousine Commission (TLC) has taken measures to prioritise the benefits of the owners of those yellow taxi medallions, most notably the 2019’s cap on registered for-hire vehicles and reduction on their no-passenger roaming time 2, the yellow taxi drivers still face heavy competition from alternative modes of transportation such as the popular New York Subway. With the release of the annual TLC Yellow Taxi Trip dataset since 2009, there has been an increasing number of literatures on using this open data to identify the factors that can improve the profitability of taxi drivers. Haggag, McManus and Paci (2017) 3 proposed that pick-up and drop-off locations constitute the “neighborhood-specific experience” that helps experienced drivers optimize their routes; while Hochmair (2016) 4 suggested that different time of the day affects passenger’s preference of taxi versus public transport which affects taxi demands. Despite the use of the dataset from different years, most of the literature reported a similar pattern in taxi usage, particularly the increased in taxi trips taken during peak hours, a strong correlation between fare amount and tip amount typical of the American gratuity culture, and the significant difference in demands between Manhattan, airports and other areas in NYC 5. Phase I of the project aims to investigate the patterns between pick-up locations, time and the profitability of taxi trips against the competition of public transportation using the latest 2019 dataset released by TLC. Through geovisualising the datasets, the findings of this exploratory data analysis hope to reaffirm the abovementioned trends for trips taken in 2019, as well as identifying the characteristics of a profitable driving strategy. Jiang, S., Chen, L., Mislove, A., &amp; Wilson, C. (2018). On Ridesharing Competition and Accessibility. Proceedings Of The 2018 World Wide Web Conference On World Wide Web - WWW ’18. doi: 10.1145/3178876.3186134↩︎ Mayor de Blasio Puts Into Effect For-Hire Vehicle Cruising Cap and Extends License Cap. (2020). Retrieved 30 August 2020, from https://www1.nyc.gov/office-of-the-mayor/news/384-19/mayor-de-blasio-puts-effect-for-hire-vehicle-cruising-cap-extends-license-cap↩︎ Haggag, K., McManus, B., &amp; Paci, G. (2017). Learning by Driving: Productivity Improvements by New York City Taxi Drivers. American Economic Journal: Applied Economics, 9(1), 70-95. doi: 10.1257/app.20150059↩︎ Hochmair, H. (2016). Spatiotemporal Pattern Analysis of Taxi Trips in New York City. Retrieved 30 August 2020, from https://doi.org/10.3141/2542-06↩︎ Correa, D., Xie, K., &amp; Ozbay, K. (2017). Exploring the taxi and Uber demand in New York City: An empirical analysis and spatial modeling. In 96th Annual Meeting of the Transportation Research Board, Washington, DC.↩︎ "],
["1-1-dataset.html", "1.1 Dataset", " 1.1 Dataset 1.1.1 TLC Yellow Taxi 2019 Data This visual report uses the 2019 subset of the Yellow Taxi trip data provided by TLC 6, with a total of over 80 million instances with 17 features. The full dataset, which is available as monthly CSV files, was pre-processed in serialisation with Pandas and PyArrow and later aggregated for visualisation and analysis. While this allows for a monthly comparison between trip records, the report focuses on analysis with an hourly temporal resolution, which benefits greatly from the representativeness of big data. Assuming the seasonality effect on taxi usage is consistent, the report findings in the hourly scale are generalisable for other years without any anomalies. The spatial temporal of the dataset is set to the 131 taxi zones table (having excluded the 2 Unknown IDs) that are also provided by TLC. 1.1.2 MTA Static Transport Accessibility Data The locations of the NYC subway stations in geodatabase format is obtained via NYC OpenData 7, which are then mapped to the corresponding taxi zones. Static subway schedule data for 2019 are available through General Transit Feed Specification (GTFS) from MTA 8. Both datasets are used to calculate the accessibility of public transport at hourly intervals during the day for each taxi zone, and include data for the NYC subway system as well as three commuter rails (Long Island Railroad, Metro-North Railroad, and Staten Island Railway) within NYC boundaries. 1.1.3 qri Transport Usage Data For public transport usage, the 2019 preprocessed Turnstile Count Data by qri 9 is used, which removes the need for pre-processing the raw dataset by MTA. The turnstile daily entry count is a measure of the total number of people who enter a particular subway station (excluding three commuter rails) in comparison to the number of taxi pick-ups from the TLC dataset. TLC Trip Record Data - TLC. (2020). Retrieved 30 August 2020, from https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page↩︎ Subway Stations. (2020). Retrieved 30 August 2020, from https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49↩︎ Other data, including static data. (2020). Retrieved 30 August 2020, from http://web.mta.info/developers/developer-data-terms.html#data↩︎ NYC Subway Turnstile Counts - 2019 | Dataset Published on qri.cloud. (2019). Retrieved 30 August 2020, from https://app.qri.io/nyc-transit-data/turnstile_daily_counts_2019/at/ipfs/QmduJkH9H9JQyseo1jfKaQAFoPKhZSKZ7aoPh7dW71jTXp↩︎ "],
["1-2-metrics.html", "1.2 Metrics &amp; Feature Selection", " 1.2 Metrics &amp; Feature Selection While the profit margin of taxi driving is generally proportionate to the total fare amount and is well-explained in the TLC pricing structure 10, there is currently no indicator of trip profitability (i.e., the ability to generate profit by taking a trip), which is an area of interest for most stakeholders and the main focus of this report. It comprises of many factors, such as the profitability of the pick-up zone and the availability of alternative modes of transportation. Thus, considering the complexity and dynamicity of this relative measurement, the current exploratory analysis will tackle these three areas separately using different feature combinations from the three datasets as follows: 1.2.1 Pick-up Zone Profitability One component of the profitability equation is where taxi drivers should circulate in order to pick up the most lucrative passengers. We devise the formula for the profitability of each pick-up zone \\(d\\) as the log product between the annual average of trip duration, number of trips and rate per trip from all trips within the zone. It represents the expected profit from picking up a customer in that zone, adjusted for the demand and competition level (via the number of trips) and the logistic cost (via the trip duration). \\[Z_d=\\log{(E_d[\\text{Trip Duration}] \\times E_d[\\text{Rate per trip}] \\times E_d[\\text{Number of trips}])}\\] where \\[\\text{Rate per trip (dollar/min)} = \\frac{\\text{Total Fare Amount}-\\text{ACPM}\\times\\text{Distance (miles)}}{\\text{Duration (min)}}\\] where \\(\\text{ACPM}=0.58\\), the estimated cost per miles by TLC 11. 1.2.2 Hourly Demands The Zone Profitability formula does not account for hourly demand nor the drop-off location, meaning that a trip which ends in a low profitable zone may not be desirable even if it begins in a highly profitable zone. The trip count is visualised with respect to pick-up and drop-off zones; and hourly demand, characterised by the total trip count per hour, is analysed for hotspot areas with the highest number of trips. 1.2.3 Public Transport Competition Factor Competition factor from alternative modes of transportation (i.e., public transport) is defined in two ways: the accessibility of public transport, measured by transport access time (TAT) 12 and the mode preference ratio 13. Transport Access Time (TAT) TAT measures the average time taken to the nearest subway station in a taxi zone at a specific hour \\(h\\) of the day: \\[\\text{TAT}_h \\text{(min)}=60\\times \\frac{\\text{Distance to nearest subway}}{v_\\text{walking}}+\\frac{60}{\\text{Average number of trains at } h \\text { hour}}\\] where \\(v_\\text{walking}=3.1\\text{ (miles/h)}\\), the average walking speed. Mode preference ratio Mode preference ratio describes the tendency of a zone \\(d\\) toward taxi or subway use, ranging between -1 and 1. \\(M_d &lt; 0\\) indicates a travel behavior tendency toward subway use, and \\(M_d &gt; 0\\) indicates a tendency toward taxi use. \\[M_d = \\frac{\\text{Average taxi trips picked up}_d}{\\max{(\\text{Average taxi trips picked up}})_\\forall} - \\frac{\\text{Average subway entries}_d}{\\max{(\\text{Average subway entries}})_\\forall}\\] where \\(\\max{(\\text{Average}_\\forall)}\\) denotes the highest average value out of all zones. Taxi Fare - TLC. (2020). Retrieved 30 August 2020, from https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page↩︎ Utilization Rate | NYC Rules. (2018). Retrieved 30 August 2020, from https://rules.cityofnewyork.us/tags/utilization-rate↩︎ Correa, D., Xie, K., &amp; Ozbay, K. (2017). Exploring the taxi and Uber demand in New York City: An empirical analysis and spatial modeling. In 96th Annual Meeting of the Transportation Research Board, Washington, DC.↩︎ Hochmair, H. (2016). Spatiotemporal Pattern Analysis of Taxi Trips in New York City. Retrieved 30 August 2020, from https://doi.org/10.3141/2542-06↩︎ "],
["1-3-data-preprocessing.html", "1.3 Data Preprocessing", " 1.3 Data Preprocessing 1.3.1 TLC Yellow Taxi 2019 Data The following data preprocessing pipeline was applied for each of the monthly dataset: Only 9 relevant features identified in the section above were selected, namely: trip_distance, DOLocationID, PULocationID, tpep_pickup_datetime, tpep_dropoff_datetime, payment_type, fare_amount, tip_amount, total_amount Trips with non-positive trip_distance were removed. tpep_pickup_datetime, tpep_dropoff_datetime were converted to datetime objects, and trips that have either pick-up or drop-off time that fall outside the current month were removed. A trip_duration attribute was created by subtracting tpep_pickup_datetime from tpep_dropoff_datetime and rounded to the nearest minute. Trips with non-positive trip_duration were removed. Trips with a trip_duration value of more than 720 (equivalent to more than 12 hours) were also removed, as there are two possible reasons for a trip lasting more than 12 hours: either the driver forgot to turn off his meter at the end of a shift which is an input error; or it is a highly unusual interstate trip, in which case the DOLocationID is likely to no longer be in NYC and should be excluded. Trips with either DOLocationID or PULocationID being 264 or 265 were removed, as they are unknown locations according to the taxi zone lookup table. Duplicated instances were removed. A subset on payment_type == 1 was also created with attributes DOLocationID, PULocationID, trip_duration, total_amount, tpep_pickup_datetime, trip_distance. Since only credit card tips (payment type 1) are consistently recorded, any analysis relating to profitability will use this subset. Note that the total_amount variable also encapsulates fare_amount, tip_amount and other taxes, and thus is more appropriate to represent revenue for profitability formula. After preprocessing all 12 monthly datasets, each attribute was merged to an annual Panda series and serialised into PyArrow feather format. For analysis and visualization, only the interested attributes are loaded onto memory (under 700MB per attribute), which allows for faster computation compared to using Spark. Geovisualisation was performed with the Geopandas package. Table 1: Dataset size (instances) comparison before and after preprocessing For profitability analysis, a dataframe consisting of credit-card only trip_distance, PULocationID, trip_duration, total_amount was created. A rate_per_trip attribute was created using the formula in Equation 2. The dataframe was then grouped by PULocationID and averaged the other values to calculate the zone_profitability according to Equation 1. 1.3.2 MTA Static Transport Accessibility Data The schedule and stop location datasets require no cleaning as the data is static. The preprocessing steps for this data replicates the TAT calculation process by Correa, Xie and Ozbay (2019) 14, with the differences in a shorter cell diameter and aggregation based on taxi zones instead of Neighbourhood Tabulation Areas. Firstly, a hex grid with a cell diameter of 500m was fitted over the NYC map, which means that the time taken to traverse each cell on foot is about 5 minutes. Each cell is characterized by the location of its centroid and was assigned to its nearest subway station stop_id through a Nearest-Neighbor BallTree algorithm. The haversine shortest distance between a cell centroid and its assigned station was also returned (See Figure 10a in the Analysis section below). departure_time was converted to 24-hour format and only the hour value was extracted. Values greater than or equal 24, such as “25:15”, indicate that the train has departed from its first stop on the previous day, and is now departing from its current stop at 1:15 AM on the next day. These values were also converted to standard 24-hour format and had their hour components extracted. To get the average number of trains per hour component of the TAT formula (Equation 3), the total count of trains departing from each stop_id – departure_time combination was divided by 3, since for each of these combinations, there are 3 types of schedules in the original dataset: Weekday, Saturday, and Sunday. The dataframe (11896 x 3) was converted from long to wide format, with each departure_time becoming its own column, resulting in a (496 x 24) dataframe with stop_id as the unique index for each row. The 8 extra values resulting from the pivot were due to stop 901 and 902 not having any trips between 12 AM and 4 AM, and as such were imputed with 0. The TAT per hour for each cell was calculated using the two components avg_freq and distance above. Once the TAT for each cell is determined, it is not difficult to calculate TAT per taxi zone by averaging the values across the cells included within the zone (See Figure 10b in the Analysis section below). Note that this resulted in each zone being assigned to a nearest stop, which would also be used in the preprocessing for the Transport Usage Dataset. Table 2: Final TAT dataframe (3750 rows of centroids, 496 stop IDs and 260 Zone IDs) 1.3.3 qri Transport Usage Data The turnstile data has been preprocessed by qri while maintaining the original column structure by MTA, which allowed for little data cleaning and seamless SQL joins with the two previous datasets. Note that since the number of turnstile entries is just an estimate of the actual public transport usage and is collected periodically rather than in real-time, attempts to compare it with hourly taxi usage do not hold strong statistical power. Thus, averaging methods were used in the following preprocessing to reduce variance in the comparisons albeit losing the hourly resolution from the preprocessed TLC dataframe. complex_id (the primary key for the Turnstile dataset) was converted to gtfs_stop_id (the primary key for the MTA schedule dataset which was also used for the TAT data) via a SQL join using the reference table from MTA website. Since the Turnstile dataset excludes data for the three commuter rails, a left join ensured that only stops with non-null usage data are included in the result dataframe. Each gtfs_stop_id was assigned to its containing taxi zone from the TAT data. The entries counts were then grouped by zone and taken the mean value to find the daily average number of people using public transport in each taxi zone. Daily average number of pick-ups (num_of_trips) grouped by zone from the preprocessed TLC dataframe was merged and used in tandem with entries column to calculate mode preference ratio per zone (as detailed in Equation 4). Correa, D., Xie, K., &amp; Ozbay, K. (2017). Exploring the taxi and Uber demand in New York City: An empirical analysis and spatial modeling. In 96th Annual Meeting of the Transportation Research Board, Washington, DC.↩︎ "],
["1-4-exploratory-data-analysis.html", "1.4 Exploratory Data Analysis", " 1.4 Exploratory Data Analysis 1.4.1 Overall trends of taxi trips Figure 1. Total number of trips by time of day The total distribution of taxi trips taken by hour of day is summarized in Figure 1. The trend shown in the histogram is congruent with past findings, wherein daily trip demand rises gradually starting from 6AM, peaks during the afternoon rush hour (from 5PM to 7PM) and then gradually decreases during the night. The differences in the afternoon peak-hour demand and the morning peak-hour demand can be attributed to weekend night-outs, as more people prefer to hail private taxi rides than public transport for this purpose. For location factor, total trip counts are projected to the NYC map based on pick-up zones (Figure 2). Three hotspot areas, namely Manhattan, LaGuardia Airport and JFK Airport, contribute significantly to the yellow taxi demand of NYC as expected. Affluent neighborhoods such as Park Avenue and Maddison Avenue (Upper East side) as well as tourist attraction areas like Time Square (Midtown) recorded the greatest number of pick-ups, followed closely by JFK and LaGuardia Airport pick-ups. Figure 2 Approximately 72.1% of the trips were paid by credit card (see Table 1). Among these trips, the fare amount (excluding any tips or fees) is typically between $5 and $20, except for a significant number of trips around the $52 mark (Figure 3). These trips are mostly airport pick-ups and drop-offs, which charge a flat rate of $52 per trip between JFK and Manhattan 15. A number of trips also recorded negative fare amount, which initially may seem invalid since money can only be positive. However, according to the Data Dictionary, a plausible explanation for these cases may be that there was a Dispute during the transaction and the driver ended up refunding the passenger. As there are only a few of these trips (i.e., mostly outliers according to the log box plot) and considering that this is not a rare event in the taxi industry, they will not be removed for analytical purposes. Figure 3. (Left) Box plot on log scale (Right) 1000-bin histogram The correlation matrix in Figure 4 shows that among the other numerical attributes, there is a strong positive correlation among trip duration, trip distance and fare amount. Although it is worth noting that tip amount has the strongest correlation with fare amount, which is expected from the American tipping culture, the profitability analysis will use the total amount paid as the variable for revenue as it encompasses both tip and base fare amount. The trip duration also shows a strong correlation with the fare amount, as the TLC fares are adjusted for speed to compensate drivers who are stuck in congestion 16. Figure 4 The correlation matrix also shows that pickup hour is uncorrelated with the other variables, which are used to evaluate the zone profitability as per Equation 1. This supports the assumption that zone profitability does not account for hourly demands, and thus these two areas will be analysed separately. Profitability metric The “Rate per trip” measure (Equation 2) is the equivalent of hourly wage for taxi driver. Unlike hourly wage, which is a fixed rate, “Rate per trip” takes into account the efficiency of the driver; meaning that for the same trip distance, faster drivers will earn more than slower drivers. Interestingly, the rate per trip distribution (Figure 4 right) is inversely proportionate to taxi demand by zones shown earlier in Figure 2. One explanation is that trips beginning in the non-central areas are usually longer than those that circulate Manhattan, consequently netting a larger amount of total fare. Furthermore, the duration of these trips may also be shorter than trips originated from hotspots areas, as they may go on fast tollways and are thus less likely to experience traffic congestion. Figure 5. Profitability by pick-up zones However, this does not mean that taxi drivers should expect higher profitability from picking up passengers in these areas, as “rate per trip” does not take into account the frequency of trips (i.e., taxi demand) in each zone. The “zone profitability” (Equation 1) combines both information in one metric and is visualized on Figure 4 (left). By adjusting for zone demand, zones with high “rate per trip” are no longer desirable as they are far less frequent than trips from the three hotspot areas. Areas between the two airports are also moderately profitable, suggesting that aside from the typical Airport – Manhattan routes, taxi drivers can also look forward to profit made from trips between LGA – JFK. 1.4.2 Hotspot Analysis From our initial analysis and zone profitability visualization, it is clear that roaming around the hotspot areas, namely Manhattan and the two Airports, has a distinct advantage over seeking for pickups in non-central areas. The sheer taxi demand guarantees that a strategy focusing on targeting pickups in these areas is profitable in the long run. The following analysis further breaks down the characteristics of trips that are picked-up in these areas according to time of the day. Manhattan Figure 6 A sample of 10,000 trips that were picked-up in Manhattan were randomly selected. Figure 6a shows that nearly 80% of the trips originated in Manhattan also finish in Manhattan, and only a small number of those trips are towards the airports. The Sankey Transition Diagram (Figure 7) further breaks down the flow of these trips by drop-off locations. For each of the three sections of Manhattan, nearly half of the trips begin and end within the section, suggesting that the majority of taxi trips are short haul and is consistent with the fare amount distribution identified earlier. By far, Midtown Manhattan records the highest taxi demand, and inner city drivers can be reassured that there is a relatively low chance that the drop-off location of their trip falls outside the highly profitable zones (in which case they may have to undesirably drive back to the profitable zones to roam for new pick-ups, incurring a logistical cost). Figure 7. Transition diagram depicting the proportion drop-off locations for trips beginning in each of the 6 areas. The size of the column is proportional to the number of trips beginning in that particular area. However, trips that take a driver out of a profitable zone generally net a higher total amount in returns (Figure 6b) albeit a high variance. These trips are most lucrative and common around the graveyard hours (10PM – 2AM), where public transport is more infrequent. The same scatterplot also gleans at the profitability of trips towards JFK Airport, which is generally much higher than the $52 flat rate due to high tip amount (between $60 and $80 in total). In comparison, trips towards LaGuardia Airport cost around $20 less. Trips towards both airports from Manhattan are notably only during working hours (5AM – 5PM). Airport Figure 8 shows the distribution heatmap of trips to and from the two major airports, broken down by time of the day and the origin or destination zones. While the drop-off pattern is similar for both airport, which is consistent with the findings from the scatterplot earlier, there is a clear cooldown period for pick-ups from LGA between 2AM and 5AM in contrast to JFK Airport where there is consistent pick-ups at all hours of the day. This suggests that targeting late trips to the airports are not favorable, as there is a high chance of not finding a potential pick-up after completing these trips. Although trips towards the airports from Manhattan are not very frequent as previously shown, they still account for most trips concluding at the airports, especially if the pick-up locations are from areas with high tourist density such as Midtown, Time Square and Upper Manhattan. Additionally, Yorkville and Hell’s Kitchen are the two residential neighborhoods with frequent taxi trips to and from the airports, as they are home to affluent young people with travelling as part of their working lives. The taxi zones between the airports also see an above-average demand throughout the day, supporting the findings from the log profitability choropleth map. Figure 8. Each stripe is a trip count distribution heatmap, with the horizontal axis being the 131 taxi zones in ascending order from left to right, and the vertical axis being the time of day starting at 0AM from the top. 1.4.3 Effect of Public Transport Access Preferred mode of transportation Figure 9a shows the daily average number of public transport entries as recorded by the turnstile usage data. Although non-central areas with low taxi usage were expected to see a higher public transport usage than the hotspots areas, it is not too surprising to see that the majority of trips on public transport still occurs in Manhattan. Most notably, Hell’s Kitchen attracts the highest number of commuters, while East Village also sees an average of more than 40000 public transport users a day. Consequently, the mode preference ratio is negative for these areas, suggesting a tendency towards public transportation, as compared to other Manhattan areas where passengers prefer taxi over subway (Figure 9b). Despite this, the numbers of taxi trips hailed from these two areas remain as high as the rest of Manhattan (Figure 2), which indicates that there is little competition between the subway and taxi industry in Manhattan as there are sufficiently high demands for both modes of transportation to cater for passengers. On the other hand, there is no subway connection to the two airports, which effectively limits potential taxi competitors to just personal vehicles and other share-riding services. Moreover, the lack of any clear mode preference in the non-central areas, coupled with abovementioned relatively low usage in both public transport and yellow taxis, suggests that residents in these areas may travel in their own cars. Further investigation into these alternative modes of transportation would benefit a big picture understanding of the transportation habits of NYC residents. Figure 9 Transport Access Time (TAT) The TAT measure offers a possible explanation to why residents would prefer public transport over taxis, as a zone with a low TAT means that public transport is more accessible and is a cheaper alternative to hailing a taxi. While TAT correlates strongly with the distance to the nearest station (Figure 10), areas with low TAT still overlaps with areas with a higher preference for taxis, especially in Manhattan. This suggests that accessibility does not play an important role in deciding the mode of transportation to use, and that most New Yorkers still prefer taxis for their convenience and higher privacy. Figure 10 Taxi car and van service. (2020). Retrieved 30 August 2020, from https://www.jfkairport.com/to-from-airport/taxi-car-and-van-service↩︎ Taxi Fare - TLC. (2020). Retrieved 30 August 2020, from https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page↩︎ "],
["1-5-conclusion.html", "1.5 Conclusion", " 1.5 Conclusion The report identifies that the profitability of driving the yellow taxi in NYC is mostly affected by the pickup hour and location, which are the two major factors affecting taxi demand. In this aspect, the findings correspond with past literature, as they suggest that in order to maximise driving profitability and earning efficiency, the drivers should: Take on consecutive short trips around Manhattan, even during traffic hours, since the fare amount will compensate for congestion, and there is a much higher probability that they can continue picking up passengers as soon as a trip is finished due to the high demand of taxis within the area Avoid late night trips towards the airports despite the high fare amount, since there is no surge pricing benefits for taxi drivers and a low probability of a potential pick-up at the airport at night Consider off-peak hour trips from the surrounding suburbs, which is an efficient choice for accumulating large sums within a shorter duration despite not being very often The report also investigates public transport as a competitor factor for transport demand. However, the findings show that there are sufficiently high demands for both modes of transportation to cater for passengers, and there might be other travel preference for residents from non-central areas. Future considerations should include for-hire vehicles, such as Uber and Lyft services, personal cars, and public transport other than subway (e.g., buses or Long Island railway) in their attempt to obtain a big picture understanding of the transportation habits of NYC residents. While the geospatial visualisations offer some interesting findings that lay the grounds for a driving strategy that is optimized for high profitability, no rigorous statistical testing or modelling was carried out to assert these suggestions. Future improvement should consider implementing these to serve as a stronger basis for conclusions and yield more effective strategies. "],
["2-phase2.html", "Phase 2 Explanatory Modelling", " Phase 2 Explanatory Modelling The visualisation phase of the project “Exploring Yellow Taxi Profitability in New York City: A Spatio-Temporal Analysis” has found that the profitability in 2019 is mostly related to the pickup hour and location, which are the two major factors affecting taxi demand. Specifically, even though other suburbs reported longer trip durations on average and consequently higher total fare amount, the three hotspot areas – Manhattan, LaGuardia Airport (LGA) and JFK Airport (JFK), had the better advantage of a surgically high demand for taxi all day long. Furthermore, the effect of competition from subway as an alternative mode of transport is also investigated through an attempt to measure the average transport preference for each suburb. This factor, however, does not seem to significantly impact taxi demand, and it is concluded that there are sufficiently high demands for both modes of transportation to cater for passengers. The second phase of the project will focus on quantifying the relationship of the abovementioned factors through explanatory modelling on the 2019 TLC dataset. In phase 1, our main outcome variable was the expected zone profitability metric, which represents the expected profit from picking up a customer in that zone, adjusted for the demand and competition level (via the number of trips) and the logistic cost (via the trip duration). A major drawback of this metric is that it has been derived and averaged out at the zone level, and thus rendering it unsuitable for modelling at the individual trip level. For measuring profitability at the individual trip level, another metric called the rate per trip was introduced as: \\[\\text{Rate per trip (dollar/min)} = \\frac{\\text{Total Fare Amount}-\\text{ACPM}\\times\\text{Distance (miles)}}{\\text{Duration (min)}}\\] where \\(\\text{ACPM}=0.58\\), the estimated cost per miles by TLC. As Total Fare Amount is highly linearly correlated with the other two factors, with pairwise Pearson’s correlations of 0.81 and 0.95 correspondingly, the proposed metric can be estimated with Total Fare Amount. As such, from a modelling perspective, Total Amount is the main random variable of interest which can both represent profitability at the trip level and quantify the relationship of the trip factors. Following phase 1 results, two time-independent hypotheses are proposed Total Amount is higher for trips starting and ending in hotspot areas. Total Amount is higher for trips starting in areas with lower public transport accessibility. Data preprocessing is performed in Python. Feature analysis and modelling is performed in R, the latter of which utilizes the lm and lmer functions in the LME4 package (see Explanatory Modelling section for more information about the models used). "],
["2-1-dataset-sampling.html", "2.1 Dataset &amp; Sampling", " 2.1 Dataset &amp; Sampling The second phase of the project uses the 2019 subset of the Yellow Taxi trip data that has been preprocessed during phase 1 with 8 relevant features, including: trip_distance, DOLocationID, PULocationID, trip_duration, total_amount, pickup_hour, tip_amount, fare_amount. As Total Amount encapsulates both Fare Amount and Tip Amount and that only credit card tips are consistently recorded, the following analysis will only consider the subset of 59360231 credit-card only instances. 2.1.1 Feature Engineering Area During phase 1, the grouping of zone locations into 6 areas (Downtown, Midtown, Uptown, LGA, JFK, Others) has allowed for better comparisons between zones, specifically for those that are in Manhattan. As such, the current analysis will also group DOLocationID and PULocationID into DOArea and PUArea, effectively introducing 2 different spatial resolution to compare the models. Transport Access Time (TAT) Despite being found not significant, the effect of public transport demand was only quantified as a daily average value in the visualization report due to the lack of tools to visualize hourly demand as measured by TAT. Meanwhile, TAT is originally used to measure the average time taken to the nearest subway station in a taxi zone at a specific time of the day: \\[\\text{TAT}_h \\text{(min)}=60\\times \\frac{\\text{Distance to nearest subway}}{v_\\text{walking}}+\\frac{60}{\\text{Average number of trains at } h \\text { hour}}\\] Explanatory modelling allows for the inclusion of time-specific TAT as a factor, which can reveal more information about how public transport demand affect Total Amount. Consequently, TAT is merged to the working dataset based on the pickup hour and pickup zone for each trip in the dataset. 2.1.2 Sampling Compared to predictive modelling where the goal is to build an accurate model, explanatory modelling does not require as many training data points as predictive modelling, provided that the results have a scientifically meaningful and statistically significant inference. Thus, the second phase of the project does not use all the 59 million data points to train the models since it will increase the training time and limit the number and complexity of the models to investigate. One million instances are randomly sampled without replacement from over 59 million data points in the chosen dataset. A Kolmogorov–Smirnov test 17 is performed for each feature of the sample to ensure that its sample distribution is not statistically different from the original distribution. The obtained sample is further randomly split into 80000 training instances and 20000 testing instances. Some of the tasks in the following analysis will not use all the 80000 training instances due to computational cost and quality of visualizations, but rather a sub-sample of this set. Specifically, the pairwise plots use a sub-sample of size 100, whereas the initial phase of building models use a sub-sample of size 10000. The final model will be trained on the full training set and evaluated with the full testing set. Lilliefors, H. W. (1967). On the Kolmogorov-Smirnov test for normality with mean and variance unknown. Journal of the American statistical Association, 62(318), 399-402.↩︎ "],
["2-2-feature-transformation.html", "2.2 Feature Transformation", " 2.2 Feature Transformation 2.2.1 Log Transformation of Numerical Features During phase 1 analysis, it has been found that most of the trip-related numerical features (except for pickup hour) have extremely high kurtosis and skewedness, which is mainly due to the disproportionality in demand between the hotspot and non-hotspot areas. One remedy is to take the log base 10 transformation for all the numerical features, including trip_duration, trip_distance, total_amount, tip_amount, fare_amount. Instances with a negative value in any of these five numerical features were discarded, and a small constant of 0.001 was added to all instances prior to log transformation to avoid taking the log of zero. As explained in the visualization report, the few negative values in the amount features may indicate refunds or transactional disputes, which does not add any statistical importance to the two proposed hypotheses, and thus will not be considered in this analysis. This transformation pipeline results in a training set of 799999 instances and a testing set of 199999 instances. From this point onwards, we will only refer to the log-transformed value of the attribute using its name (e.g., “Duration” refers to log10 of trip_duration). Additionally, no transformation is applied to TAT to preserve its theoretical meaning. 2.2.2 Factorization of Categorical Features Categorical features, namely DOLocationID (250 levels), PULocationID (250 levels), DOArea (6 levels), and PUArea (6 levels), are transformed into R factor objects. pickup_hour is dynamically treated as either a numerical feature (not log-transformed) or a categorical (factor) feature depending on the specific use case. "],
["2-3-feature-analysis.html", "2.3 Feature Analysis", " 2.3 Feature Analysis 2.3.1 Feature Distribution Figure 1. Feature log distributions Figure 1 shows the distributions of the main six features to be included in the modelling phase. The distributions, which come from the training set, demonstrate similar trends in the distributions visualized in phase 1 of the project, specifically: Trip distance and Total amount display bimodality (a and b), with the lower mode corresponding to the airport trips. These trips typically cost around $52, or equivalently 1.72 in log scale. More than a quarter of trips have a log Tip amount of -3 (c), which means no tips were included in the original transactions. The missing gaps in the duration distribution (d) are due to the nearest integer rounding when this feature was engineered in phase 1. As such, the shape of the distribution is not abnormal. Note that the bimodality due to airport trips is not strongly exhibited for this feature. Even without a choropleth map, the demand in the three hotspot areas is transparent from the raw count of trips per zone (e and f). Areas with high pick-up demand usually imply high drop-off demand correspondingly. It is also worth noticing that none of the transformed distribution for the numerical features (Distance, tip amount, total amount, duration) are approximately normal. Two tests for normality, D’Agostino’s K-squared test 18 and Shapiro–Wilk test 19, are performed on each feature in both the training and the testing sets using the scipy Python package and detected no normality with extremely high statistical significance. This insight helps with the choice of models in the modelling section below. 2.3.2 Pairwise Correlation Figure 2. Pairwise scatterplot matrix colored by pickup areas A sample of 100 points from the training set was plotted on a pairwise scatter-plot matrix, colored by pickup areas (Figure 2). As expected, all numerical features have statistically significant multicollinearity with each other, barring the instances with no tip. As our target variable is Total amount, it means that only one of Distance, Fare Amount, Tip Amount, and Duration should be included in the model so that coefficient estimates are not affected by scaling (see Feature Selection below). Furthermore, Total amount seems to be randomly distributed across different zones and pick-up hours, suggesting that these features may not be suitable predictors for the model. TAT is moderately linear to Total amount and shows clear interaction with pick-up areas from the coloring. Specifically, TAT and Total amount for airport areas are noticeably higher that the other areas. 2.3.3 Comparison of Spatial Resolution While visualizing the data on a zone-based resolution is effective in phase 1, the high cardinality of 250 different zones presents a challenge in model training. As such, a broader segmentation of location is proposed by grouping the zones into 6 areas: Downtown, Midtown, Uptown, LGA, JFK, Others. Although the initial plan was to compare the effectiveness of these two grouping strategies, the curse of dimensionality 20 introduced by the number of zones bore an immense time complexity cost on training even the minimal dataset. Thus, this analysis is done pre-emptively to ensure that area-based grouping will not lose much information compared to zone-based grouping. As the analysis lends itself to the idea of information gain from information theory, the normalized mutual information (NMI) will be used as the metric for comparison 21. The Total amount is cut to 100000 bins, before being used to calculate the NMI with each of PULocationID, PUArea, DOLocationID, DOArea. While area-based grouping indeed results in information loss (see the NMI result table above), the trade-off between the loss and the reduction in training time as well as model simplicity is well-worth the use of area-based grouping over zone-based grouping strategy. 2.3.4 Categorical – Numerical Feature Interactions Figure 2 implies that Total amount is somewhat affected by areas. Specifically, the median values for the airports are higher than the remaining areas, suggesting a higher slope for these two areas. However, they also recorded the largest number of outliers despite having a relatively lower IQR spread, which means that the mean error from predictions will be expectedly higher than the other areas. Such interaction is statistically supported using the Kruskal-Wallis test by ranks, a non-parametric equivalence of the one-way ANOVA 22, on the 100-instance subsample \\((\\chi^2(5)=22.413, p=.0004)\\). Furthermore, the boxplot also raises a concern about malicious data points that may distort the model fit. A subjective threshold of 2.3 (equivalently a trip total amount of $200) was used as a cut-off to rule out any potential outliers to the training set. An investigation into these outliers indicates that, while some are genuinely due to long trip distance and duration, there are some highly generous customers whose tips are nearly tenfold the fare amount. As these trips do not honour the typical relationship with Total amount, we further remove trips with outlier tip-to-total ratios from the training set. As a result, 799925 instances remain in the training set. 2.3.5 Feature Selection for Explanatory Models From the feature analysis, we rule out PULocationID, DOLocationID and pickup_hour from the predictors pool. Amongst the numerical features, Trip Duration seems to be the most suitable one, as a driver can usually estimate how long the trip will take prior to picking up the passengers. Trip Distance can be estimated with the interaction between PUArea and DOArea, while Fare Amount and Tip Amount are only known after the trip has taken place, and thus do not make sense to be included in a regression model. The set of predictors X for the explanatory modelling phase is finalized as: Duration, PUArea, DOArea, TAT. d’Agostino, R. B. (1971). An omnibus test of normality for moderate and large size samples. Biometrika, 58(2), 341-348.↩︎ Razali, N. M., &amp; Wah, Y. B. (2011). Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics, 2(1), 21-33.↩︎ Verleysen, M., &amp; François, D. (2005, June). The curse of dimensionality in data mining and time series prediction. In International work-conference on artificial neural networks (pp. 758-770). Springer, Berlin, Heidelberg.↩︎ Strehl, A., &amp; Ghosh, J. (2002). Cluster ensembles—a knowledge reuse framework for combining multiple partitions. Journal of machine learning research, 3(Dec), 583-617.↩︎ Kruskal, W. H., &amp; Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American statistical Association, 47(260), 583-621.↩︎ "],
["2-4-model-selection.html", "2.4 Model Selection", " 2.4 Model Selection The goal of explanatory modelling is to provide insights into each explanatory variable rather than to arrive at a complex black-box model with high predictive accuracy. Therefore, the proposed methodology is to compare the performance of different variations of the same family model and attempt to pinpoint which variables are responsible for the differences in the predicted values for Total Amount. The Linear Model (LM) family is chosen as it is easy to interpret, efficient to train with very low chance of non-convergence, and robust to mixed-type inputs (ANOVA regression if there are categorical inputs, as well as ability to include interaction terms). An extension of LM – the Linear Mixed Models (LME), are also considered. 2.4.1 Linear Additive Models In the additive LMs, the relationship between response variable to be predicted (Total Amount) and the set of predictors X (assumed to be independently affecting the response variable) can be expressed as: \\[\\log{(\\text{Total amount})}=X\\beta + \\varepsilon\\] For each categorical predictor of \\(n\\) factor levels, the coefficient vector \\(\\beta\\) introduces \\(n – 1\\) additional terms, each of which corresponds to the contrast with the first factor level. The error vector \\(\\varepsilon\\) is assumed to be normally distributed. 2.4.2 Linear Interaction Models From the results of the Feature Analysis section, it is clear that the predictors exhibit some levels of collinearity and interactions, violating the assumption of the additive models. Questions such as “will there be a difference in Total Amount between a trip beginning in Downtown and ending in JFK compared to a trip beginning in Uptown and ending in JFK?” can be answered with the interaction models, which introduce a \\(\\beta_*\\) interaction coefficient vector and an interaction design matrix \\(X_*\\): \\[\\log{(\\text{Total amount})}=X\\beta + X_* \\beta_* + \\varepsilon\\] The interaction design matrix \\(X_*\\) can handle categorical – categorical, numerical – categorical, and numerical – numerical interactions. Note that for categorical interactions, the number of additional coefficients increases in proportion to the number of factor levels. Hence, the use of Area-level spatial resolution is significantly more computationally efficient than Zone-level resolution as the interaction between drop-off zones and pick-up zones can introduce up to 62500 terms in the model! 2.4.3 Linear Mixed Models 23 LMEs are also used to handle interactions between variables, by additionally modelling the effect of the numerical predictors as random variables (called “random effects”) for each factor level in some categorical predictors. \\[\\log{(\\text{Total amount})}=X\\beta + Zu + \\varepsilon\\] For example, from the Feature Analysis section, the Total Amount for trips finishing at JFK and LGA is generally greater than other trips. This between-group relationship is captured by the fixed-effect coefficients in \\(\\beta\\), which is similar to the classic LMs. However, among the trips finishing at JFK, the ones that begin from areas closer to JFK will have lower Total Amount than the ones begin from further areas. This within-group relationship is additionally modelled by the random-effect coefficients in \\(u\\), and \\(Z\\) is the design matrix for the within-group random effects. LMEs are generally similar to the one-factor LMs. A random intercept LME begins with a different mean of Total Amount for each group, whereas a one-factor model implies that all groups have the same mean Total Amount (i.e., the intercept) and models the effect of being in a specific group on the intercept. Note that LMEs do not have a standard degrees-of-freedom measurement as it is effectively a nested LM within another LM. 2.4.4 Experimental Design Table 3: Experimental design A benchmark model M0 is first established by running an AIC-based stepwise feature selection on the largest-possible interaction model of all 4 predictors (Duration + TAT + PUArea + DOArea). As we are not interested in maximizing predictive power but rather the effect of each term in the model, the experimental design (Table 3) mostly considers models of a smaller scope than M0 (except for M9). The description column highlights the feature of interest in each model, as well as the type of model used (additive, interactive, or mixed model). The initial phase of model building is to investigate how closely the performance of the alternative models M1 – M9 come to the benchmark level, as well as comparing the performance level of related additive, interaction and mixed models to identify which one is most suitable to describe the relationship between each predictor and the target variable. The second phase of model building is to train the best model on the full training set, evaluate with the testing set and perform error analysis. 2.4.5 Model Selection Metrics Akaike Information Criteria (AIC) To compare performance between LMs and LMEs, the traditional goodness-of-fit measure R2 is not suitable due to the structure of LMEs. As such, the preferred metric for model comparison is Akaike Information Criteria (AIC), which is defined as \\[AIC=2*\\text{Number of parameters} - 2\\log{\\hat{L}}\\] An advantage of AIC is that it deals with the trade-off between the goodness of fit of the model and the simplicity of the model by penalizing complex models. The lower the AIC of the model, the better it is in comparison to other models trained on the same dataset. We also utilize Model Relative Likelihood – a generalized comparison ratio version of AIC, to compare each alternative model with the benchmark model M0. \\[MRL_i = \\exp{\\left(\\frac{AIC(M_0)-AIC(M_i)}{2}\\right)}\\] A MRL closer to 1 indicates that the model performs generally the same as M0, whereas a MRL=0 means that the model is comparatively different from M0. Root Mean Squared Error (RMSE) Each model will also be evaluated with predictive performance based on its prediction of the testing set. The traditional Root Mean Squared Error (RMSE) is calculated for each model. For the current regression task, the lower the value of RMSE, the better the model performs. \\[RMSE = \\sqrt{\\frac{1}{N_\\text{test}}\\sum_1^{N_\\text{test}}({\\text{Predicted}-\\text{Actual}})^2}\\] Note that the reported RMSEs have been exponentially transformed so that they can be interpreted as the actual differences from the actual Total Amount in dollar values. Gałecki, A., &amp; Burzykowski, T. (2013). Linear mixed-effects model. In Linear Mixed-Effects Models Using R (pp. 245-273). Springer, New York, NY.↩︎ "],
["2-5-modelling-results.html", "2.5 Modelling Results", " 2.5 Modelling Results 2.5.1 Benchmark Model Term Relevance Analysis Table 4: Benchmark Model M0 ANOVA Results Table 4 reports the ANOVA analysis on the resulting coefficients of the AIC-based stepwise selection on the largest possible interaction model from the 4 selected predictors. As expected, there is a strong presence of interaction between Duration and Areas, especially the influence of drop-off areas on the trip duration. This result is not surprising since the destination should logically have a higher impact on the duration of the trip than the origin. This effect is also present in the direct effect of Areas on Total amount itself (the non-interaction terms), where the F-value for DOArea is observably higher than for PUArea and can be explained in a similar fashion. Comparing the F-values, however, only provides a relative indicator of which location information affects the Total amount and Duration more, rather than the magnitude of such effects. Nevertheless, the results imply that area-based grouping is efficient in retaining relevant information about the linear relationship at a cost of just 5 extra degrees of freedom. Another interesting is that although TAT has a significant relationship with Total amount, it does not interact with other features. One explanation for the non-interaction is that TAT itself has already embedded location-related information (as well as time-related information yet not clearly seen due to the omission of the Hour feature). On the other hand, the pairwise scatterplot also did not pick up on any significant relationship between TAT and Duration. 2.5.2 Model Comparison on the 10000-Instance Subsample The table and grouped bar plot above present the evaluation for the performance of all 10 models (recall that we are looking for lower values for both AIC and RMSE, so the darker the column implies a better fit of the model). Overall, although none of the alternative models surpasses the benchmark model in both AIC and RMSE, the differences in RMSE is trivial if rounding to the nearest cent is applied with each model’s average error is around $1.21 on any given prediction. Moreover, the AIC is consistent with RMSE, and the model relative likelihood column full of zeroes further highlights the benchmark superior performance over the alternative models. As such, the only model that will be further investigated is the benchmark M0. Effect of Duration Figure 3. Standard diagnostics plot for M0 and M1 The baseline model M1, despite recording the worst RMSE, isolates and reveals how strongly Duration drives the final Total amount of a trip. Compared to M1, the predictive power of the best-performing model M0 only increases by approximately $0.1 at the cost of 46 extra terms in the model, implying that as expected, the single major driver for the fare amount is the duration of the trip. However, the standard diagnostic plots for M1 (Figure 3) see some noticeable trends in the error distribution of the predictions, which violate the assumptions of a Linear Model. While this does not rule out the linear relationship between the two variables, it shows that a linear fit on Duration tends to overestimate Total amount, especially for trips totaling under $10 and more than $30. Compared this to the diagnostic plots for M0, the additional terms and interactions show significant improvement in “linearising” the predictions. Effect of Areas Perhaps two of the most significant results from Table 2 is the performance increase from M1 to M2 as we added the Area terms, and from M2 to M4 as we changed the relationship from additive to interactive. The additive improvement supports the findings from the Feature Analysis, as the baseline costs (i.e., the intercepts) for trips beginning in one area are different from another. While Duration is highly correlated to Total amount, the inclusion of Area terms helps to differentiate between long duration – short distance trips (e.g., trips in traffic jams) and long duration – long distance trips, as we opted not to include Distance as a predictor due to multicollinearity. The different factor levels in PUArea and DOArea serve as a rough estimate of trip distance, which can explain why performance improves. A visualization of the additive effect, as well as its mixed model equivalence (M7), is demonstrated in Figure 4. Figure 4. Model fits (the dotted lines are LMEs and dashed lines are LMs). Note that LMEs and LMs are the same here. Meanwhile, the interactive model M4 is the second-best model notwithstanding M0, which further highlights the effects of different PUArea - DOArea combinations, rather than assuming that each area only individually affects Total amount. The intercept term for this model can be interpreted as a baseline distance, with 36 possible values, hence provides more information about the Total amount. Effect of interactions between Duration and Areas M5 provides information about how different Areas change the effect of Duration on Total amount. Not surprisingly, the interactive model M5 does not perform as well as M4, as the information about distance is not captured without the interaction between PUArea and DOArea. Effect of TAT Evidence from comparing M2 with M3 and M0 with M6 shows that the effect of TAT on Total amount is not as noticeable as other predictors, as it only increases predictive power by under $0.01. This is not surprising following the results from Phase 1, where it has been found that availability of public transport does not strongly influence the demand for taxi. Nonetheless, TAT remains a relevant predictor in the benchmark model with high significance (Table 4) and therefore will not be ruled out in the final model. Random Effects As mentioned in the Model section, LMEs can be thought of as a slightly improved version of LMs with more structured in its relationship. Therefore, it is expected that if the random effects are modelled correctly, the LMEs perform at least as well as its corresponding LM equivalents (i.e., treating the random effects as fixed effects) in terms of RMSE. Equivalent pairs in the current design are M2 – M7, M4 – M8, M3 – M9. The results show that for the current relationship, modelling Duration or TAT as random effects based on different Areas does not provide any significant improvement or even a different result (Figure 4) and thus interactive models are a better option. The result also implies that Duration and TAT does not seem to follow any specific distribution within each factor level of Areas, confirming good feature selection without any latent correlation. 2.5.3 Final Model Inference The benchmark model M0 is trained on the full training set and evaluated on the full testing set. With a training result of \\(R^2 = 0.8965\\) and \\(RSE = 10^{0.07249} = 1.18\\), the final model does not vary much from the sampled model during evaluation, recording a \\(RMSE = 1.183\\). The stability of the results confirms the effectiveness of the sampling strategy used, as well as the validity of using the final model to infer the relationship between Total amount and the predictors. The reverse-transformed regression results are recorded in Table 5, which sheds light on the magnitude of the abovementioned effects from the various predictors. Using a fixed-variable cost model, with an increment of 10 minutes in trip duration, we can make simplified interpretation of the regression as: \\[\\text{Total amount} = \\text{Fixed base rate} + \\text{Variable rate} * \\text{Duration}\\] Note that since the interaction model implies a different base rate for each PUArea-DOArea combination, it does not make sense to analyse the coefficients in separation. For example, a LGA-bound trip from Midtown with a fixed base rate of $15.78 and variable rate of $2.19 does not necessarily net a higher total amount than a LGA-bound trip from Downtown with a fixed base rate of $14.36 as the latter has a higher variable rate of $2.26. Table 5: Reverse-transformed regression results of the final model Interpretations of fixed &amp; variable rates Overall, variable rates are higher for the non-hotspot areas as they cover a much wider number of zones than the other five areas (note that the variable rates can assume an analogous role of a standard deviation), whereas airport-related trips enjoy an expectedly higher base rate. Trips within Manhattan have a base rate of only around $4 to $5, and for every 10 minutes within this metro area, the taxi drivers can average between $2.5 to $3. Trips around the Uptown zones have a higher base rate than in Midtown and Downtown, while the variable rates are around $0.5 higher in the latter two areas. One explanation could be due to the higher traffic around the tourist attraction areas such as Time Square, Rockefeller (Midtown) and Soho, Tribeca (Downtown), which may incur higher travelling time. This finding also highlights the advantage of taxi drivers over Uber drivers when driving in Manhattan, as Uber fees are distance-based rather than duration-based, thus do not protect drivers from being disadvantaged in high traffic situation. However, not all regression coefficients in the model are fully comprehensible. Particularly, base rate estimates for the JFK-JFK and LGA-JFK cases are unusually high, at more than $100 for trips that supposedly begin and end at the same area. A closer look at these trips in the training set reveals that the fare amount of these trips is indeed incredibly high despite the short distance, and hence potentially fraudulent or erroneous trip records in the data collection stage instead of model-related issues. Interpretation of TAT effects Since there is no interaction term for TAT, a regression coefficient of 1.01 means that taxi trips will cost $1 more for every 1 minute further away from the nearest subway station. 2.5.4 Error Analysis Figure 5 summarises the distribution of reversed-transformed prediction errors for each PUArea – DOArea combination on the 199999 test instances. Trips picking up in Downtown and Midtown seems to have the most consistent predictions, while the airport trips have a much higher error range which seems to stem from the regression coefficient issues discussed earlier. The relative length of the whiskers also corresponds to the variable rates related to each combination, further cementing the validity of the coefficient inferences. While the results look approximately consistent across the areas, it is worth noticing that we did not extend the scope of error analysis to the outliers in the distribution, some of which are as extreme as $10 off the actual values. Figure 5. Error Analysis Future work should allow for outlier analysis to reveal even more information about the more extreme trips in which the ordinary relationship between pick-up area, drop-off area, duration, TAT and total amount is not honoured due to special circumstances. "],
["2-6-conclusion-pt2.html", "2.6 Conclusion", " 2.6 Conclusion In summary, phase 2 of the project did not find sufficient evidence to support the claim that Total Amount is higher for trips starting and ending in hotspot areas, as only the airport areas follow such trend. However, Manhattan trips are more consistent in their price range, and coupled with our knowledge from phase 1 findings about the overwhelmingly high demand of taxi in this area, there is no reason for taxi drivers not to target the core of Big Apple as it will yield consistent income despite a low per-trip amount. The modelling also supports the correlation between lower accessibility to public transport and taxi price, implying that Manhattan areas that are not directly adjacent to subway are potentially great pick-up areas for taxi drivers. These analyses constitute a first step toward quantifying the relationship between factors affecting taxi profitability using a self-explanatory glass-box model in place of complex black-box models. Aside from minor recommendations throughout the analysis, a major drawback of the analysis is the exclusion of the time and zone attributes from the model due to limited resources. Future work should investigate the relationship using a time-dependent model such as a variation of the LME introduced in this report with a more refined resolution of temporal and spatial structure to further reduce the error range, effectively improving the strength of the inferences. "],
["code-section.html", "Code Section", " Code Section Main tools used: Python: pandas, geopandas, scipy, pyfeather, matplotlib, plotly R: lme4, plotly, dplyr, tidyr "],
["2-7-data-preprocessing-python.html", "2.7 Data Preprocessing (Python)", " 2.7 Data Preprocessing (Python) 2.7.1 Phase 1 import numpy as np import pandas as pd from datetime import datetime, timedelta import os def preprocessing(path, month, save_name, sample=False, profiling=False): print(&#39;FILENAME:&#39;,path) if sample: df = pd.read_csv(path, index_col=0).reset_index(drop=True) else: df = pd.read_csv(path) n_rows = df.shape[0] # Original number of rows print(&#39;[0] Number of trips in the raw dataset:&#39;, n_rows) # Feature selection extraction = [&#39;trip_distance&#39;, &#39;DOLocationID&#39;, &#39;PULocationID&#39;, &#39;tpep_pickup_datetime&#39;, &#39;tpep_dropoff_datetime&#39;, &#39;payment_type&#39;, &#39;fare_amount&#39;, &#39;tip_amount&#39;, &#39;total_amount&#39;] df = df[extraction] print(&#39;[1] Select features&#39;) # Remove trips with non-zero distance zero = df[df[&#39;trip_distance&#39;]&lt;=0].shape[0] print(&#39;[2] Remove trips with non-positive distance:&#39;, zero) df = df[df[&#39;trip_distance&#39;]&gt;0] # Convert to datetime df[&#39;tpep_pickup_datetime&#39;] = pd.to_datetime(df[&#39;tpep_pickup_datetime&#39;]) df[&#39;tpep_dropoff_datetime&#39;] = pd.to_datetime(df[&#39;tpep_dropoff_datetime&#39;]) # Check datetime range d1 = datetime(2019, month, 1) if month==12: d2 = datetime(2020, 1, 1) else: d2 = datetime(2019, month+1, 1) # Check wrong date / month wrongdate = df[(df[&#39;tpep_pickup_datetime&#39;]&lt;d1) | (df[&#39;tpep_dropoff_datetime&#39;]&gt;=d2)].shape[0] df = df[(df[&#39;tpep_pickup_datetime&#39;]&gt;=d1) &amp; (df[&#39;tpep_dropoff_datetime&#39;]&lt;d2)] print(&#39;[3] Convert to date/time &amp; drop wrong dates:&#39;, wrongdate) # Feature engineer &#39;trip_duration&#39; in minutes df[&#39;trip_duration&#39;] = df[&#39;tpep_dropoff_datetime&#39;]-df[&#39;tpep_pickup_datetime&#39;] df[&#39;trip_duration&#39;] = round(df[&#39;trip_duration&#39;].dt.total_seconds().div(60)).astype(int) # Feature engineer &#39;pickup_hour&#39; as integer df[&#39;pickup_hour&#39;] = df[&#39;tpep_pickup_datetime&#39;].dt.hour.astype(int) # Remove `tpep_dropoff_datetime` df = df.drop(&#39;tpep_dropoff_datetime&#39;,axis=1) print(&#39;[4] Feature engineer `trip_duration` in minutes and `pickup_hour` as nearest integer&#39;) # Remove trips more than 12 hours (likely forgot to turn off meter) duration = df[(df[&#39;trip_duration&#39;]&gt;=720) | (df[&#39;trip_duration&#39;]&lt;=0)].shape[0] df = df[(df[&#39;trip_duration&#39;]&lt;720) &amp; (df[&#39;trip_duration&#39;]&gt;0)] print(&#39;[5] Remove trips more than 12 hours or non-positive:&#39;, duration) # Recast `PULocationID` as integer df[&#39;PULocationID&#39;] = df[&#39;PULocationID&#39;].astype(int) pu = df[(df[&#39;PULocationID&#39;]==264) | (df[&#39;PULocationID&#39;]==265)].shape[0] df = df[(df[&#39;PULocationID&#39;]!=265) &amp; (df[&#39;PULocationID&#39;]!=264)] print(&#39;[6] Recast `PULocationID` as integer &amp; remove unknown IDs:&#39;, pu) # Recast `DOLocationID` as integer df[&#39;DOLocationID&#39;] = df[&#39;DOLocationID&#39;].astype(int) pu = df[(df[&#39;DOLocationID&#39;]==264) | (df[&#39;DOLocationID&#39;]==265)].shape[0] df = df[(df[&#39;DOLocationID&#39;]!=265) &amp; (df[&#39;DOLocationID&#39;]!=264)] print(&#39;[7] Recast `DOLocationID` as integer &amp; remove unknown IDs:&#39;, pu) # Impute and categorise missing &#39;payment_type&#39; with 0 df[&#39;payment_type&#39;] = df[&#39;payment_type&#39;].fillna(0) df[&#39;payment_type&#39;] = df[&#39;payment_type&#39;].astype(int) print(&#39;[8] Impute and categorise missing `payment_type` with 0&#39;) # Drop duplication duplicate = df.duplicated().sum() df = df.drop_duplicates().reset_index(drop=True) print(&#39;[9] Drop duplicates &amp; reset index:&#39;, duplicate) # Summary print() print(&#39;&gt;&gt; Final DF shape:&#39;,df.shape) print(&#39;&gt;&gt; Reduction size (%):&#39;, (n_rows-df.shape[0])/n_rows*100) print(&#39;&gt;&gt; Missing values:&#39;) print(df.isnull().sum()) # Save print() df.to_feather(save_name) print(&#39;&gt;&gt; Saved to feather-format&#39;) # Panda Profiling if profiling: ProfileReport(df, minimal=True).to_file(output_file=save_name+&#39;_profiling.html&#39;) df = df[df[&#39;payment_type&#39;]==1].drop(&#39;payment_type&#39;,axis=1).reset_index(drop=True) df.to_feather(save_name+&#39;_creditCard&#39;) print(&#39;Credit Card only size:&#39;,df.shape) print(&#39;&gt;&gt; Saved to feather-format, credit-card only&#39;) print() print(&#39;-----------------------------------------------&#39;) def load_attribute(attr, credit_card=False): df = np.array([]) for i in MONTHS: if credit_card: path = os.path.abspath(os.path.join(&quot;taxi&quot;, &quot;ETL&quot;, i+&quot;19&quot;+&quot;_creditCard&quot;)) else: path = os.path.abspath(os.path.join(&quot;taxi&quot;, &quot;ETL&quot;, i+&quot;19&quot;)) df = np.concatenate([df, pd.read_feather(path)[attr].to_numpy()]) return df # Feature engineer ACPM = 0.58 df = {&#39;location&#39;: np.load(&#39;PULocationID_Credit.npz&#39;)[&#39;data&#39;], &#39;total_amount&#39;: np.load(&#39;total_amount_Credit.npz&#39;)[&#39;data&#39;], &#39;tip_amount&#39;: np.load(&#39;tip_amount_Credit.npz&#39;)[&#39;data&#39;], &#39;trip_distance&#39;: np.load(&#39;trip_distance_Credit.npz&#39;)[&#39;data&#39;], &#39;trip_duration&#39;: np.load(&#39;trip_duration_Credit.npz&#39;)[&#39;data&#39;]} df = pd.DataFrame(df) df[&#39;rate_per_trip&#39;]=(df[&#39;total_amount&#39;]-ACPM*df[&#39;trip_distance&#39;])/df[&#39;trip_duration&#39;] df = df.drop([&#39;trip_distance&#39;],axis=1) df[&#39;tip_rate&#39;] = df[&#39;tip_amount&#39;]/(df[&#39;total_amount&#39;]-df[&#39;tip_amount&#39;]) df = df.drop([&#39;tip_amount&#39;,&#39;total_amount&#39;],axis=1) df = df.groupby(&#39;location&#39;).mean().reset_index() df[&#39;location&#39;] = df[&#39;location&#39;].astype(int) df.to_csv(&#39;feature-engineer_by_PULocationID.csv&#39;) df.head() # 103 (Statue of Liberty Island) &amp; 110 doesn&#39;t have any trips 2.7.2 Phase 2 import numpy as np import pandas as pd import random from datetime import datetime, timedelta import os # Stage 1 - Sampling # Set seed random.seed(26) # Randomize 1 million indices ONEMILLION = random.sample(range(59360231), 1000000) # 80-20 train test split TRAIN_INDEX = random.sample(ONEMILLION, 800000) TEST_INDEX = np.setdiff1d(ONEMILLION, TRAIN_INDEX) ATTRS = [&#39;trip_distance&#39;, &#39;trip_duration&#39;, &#39;PULocationID&#39;, &#39;DOLocationID&#39;, &#39;pickup_hour&#39;] CREDIT = [&#39;fare_amount&#39;, &#39;tip_amount&#39;, &#39;total_amount&#39;] for i in ATTRS+CREDIT: print(&#39;&gt;&gt; Atribute:&#39;,i) df = np.load(i+&#39;_credit.npz&#39;)[&#39;data&#39;] train_df = df[TRAIN_INDEX] print(&#39;Train Size (MB):&#39;, train_df.nbytes/1000000) np.savez_compressed(i+&#39;_train.npz&#39;, data=train_df) del(train_df) test_df = df[TEST_INDEX] print(&#39;Test Size (MB):&#39;, test_df.nbytes/1000000) np.savez_compressed(i+&#39;_test.npz&#39;, data=test_df) del(test_df) print(&#39;-------------------------------&#39;) del(df) from scipy.stats import ks_2samp # K-S Test for checking sampling integrity for i in ATTRS+CREDIT: print(&#39;&gt;&gt; Atribute:&#39;,i) test_df = np.load(i+&#39;_test.npz&#39;)[&#39;data&#39;] train_df = np.load(i+&#39;_train.npz&#39;)[&#39;data&#39;] p = ks_2samp(test_df, train_df)[1] if p&lt;0.05: print(&#39;The test set and train set have different distribution&#39;) print(&#39;p-value for KS Test is:&#39;,p) if p&gt;=0.05: print(&#39;The test set and train set have similar distribution&#39;) print(&#39;p-value for KS Test is:&#39;,p) print(&#39;-------------------------------&#39;) del(test_df) del(train_df) # Stage 2 - Dataset Creation train = pd.DataFrame() test = pd.DataFrame() for i in ATTRS+CREDIT: train[i] = np.load(i+&#39;_train.npz&#39;)[&#39;data&#39;] test[i] = np.load(i+&#39;_test.npz&#39;)[&#39;data&#39;] NUM_COLS = [&#39;trip_distance&#39;, &#39;trip_duration&#39;, &#39;fare_amount&#39;, &#39;tip_amount&#39;, &#39;total_amount&#39;] # Filter non-negative values train = train[(train[NUM_COLS]&gt;=0).all(1)] test = test[(test[NUM_COLS]&gt;=0).all(1)] # Log transform the numerical cols (+0.001 to handle log(0)) for col in NUM_COLS: train[col] = np.log10(train[col]+0.001) test[col] = np.log10(test[col]+0.001) print(&quot;Train set shape:&quot;, train.shape) print(&quot;Test set shape:&quot;, test.shape) # Save to feather format train.reset_index(drop=True).to_feather(&#39;train_set&#39;) test.reset_index(drop=True).to_feather(&#39;test_set&#39;) # Stage 3: Feature Scaling from matplotlib import pyplot as plt import seaborn as sns plt.style.use(&#39;D:/DS/0_ASS1/stylesheet.mplstyle&#39;) ### TRIP DISTANCE DISTRIBUTION (LOG SCALE) #plt.figure(figsize=(10,5)) sns.distplot(train[&#39;trip_distance&#39;], hist=True, kde=True, bins=50, color=&#39;#86bfd0&#39;, kde_kws={&#39;linewidth&#39;: 1, &#39;bw&#39;:0.05, &#39;color&#39;:&#39;#147f9f&#39;}) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Log(Trip distance)&#39;) plt.suptitle(&quot;Log Distribution of Trip distance (miles) in Train Set&quot;, ha = &#39;right&#39;) plt.show() ### TOTAL AMOUNT DISTRIBUTION (LOG SCALE) #plt.figure(figsize=(10,5)) sns.distplot(train[&#39;total_amount&#39;], hist=True, kde=True, bins=50, color=&#39;#fed38f&#39;, kde_kws={&#39;linewidth&#39;: 1, &#39;bw&#39;:0.05, &#39;color&#39;:&#39;#f9ab17&#39;}) plt.xlabel(&#39;Log(Total amount)&#39;) plt.ylabel(&#39;Density&#39;) plt.suptitle(&quot;Log Distribution of Total amount ($) in Train Set&quot;, ha = &#39;right&#39;) plt.show() ### TIP AMOUNT DISTRIBUTION (LOG SCALE) #plt.figure(figsize=(10,5)) sns.distplot(train[&#39;tip_amount&#39;], hist=True, kde=True, bins=50, color=&#39;#c7827b&#39;, kde_kws={&#39;linewidth&#39;: 1, &#39;bw&#39;:0.05, &#39;color&#39;:&#39;#a80000&#39;}) plt.xlabel(&#39;Log(Tip amount)&#39;) plt.ylabel(&#39;Density&#39;) plt.suptitle(&quot;Log Distribution of Tip amount ($) in Train Set&quot;, ha = &#39;right&#39;) plt.show() ### TRIP DURATION DISTRIBUTION (LOG SCALE) #plt.figure(figsize=(10,5)) sns.distplot(train[&#39;trip_duration&#39;], hist=True, kde=True, bins=50, color=&#39;#abc27e&#39;, kde_kws={&#39;linewidth&#39;: 1, &#39;bw&#39;:0.05, &#39;color&#39;:&#39;#5a8303&#39;}) plt.xlabel(&#39;Log(Trip duration)&#39;) plt.ylabel(&#39;Density&#39;) plt.suptitle(&quot;Log Distribution of Trip duration (min) in Train Set&quot;, ha = &#39;right&#39;) plt.show() ### PICK UP HOURS #plt.figure(figsize=(30,20)) pickup_hour = train[&#39;pickup_hour&#39;] (unique, counts) = np.unique(pickup_hour, return_counts=True) plt.bar(unique, counts, tick_label=[str(int(x))+&#39;:00&#39; for x in unique], width=0.9) plt.xticks(rotation=70) plt.xlabel(&#39;Time of day (hour)&#39;) plt.ylabel(&#39;Number of trips&#39;) plt.suptitle(&quot;Number of trips by time of day in Train Set&quot;, ha = &#39;right&#39;) plt.show() ### PICK-UP ZONES #plt.figure(figsize=(30,20)) pickup_zone = train[&#39;PULocationID&#39;] (unique, counts) = np.unique(pickup_zone, return_counts=True) plt.bar(unique, counts, width=0.9, color=&#39;#e5a5ff&#39;) #plt.xticks(rotation=70) plt.xlabel(&#39;Pick-up Zone ID&#39;) plt.ylabel(&#39;Number of trips&#39;) plt.suptitle(&quot;Number of trips by Pick-up Zone in Train Set&quot;, ha = &#39;right&#39;) plt.show() ### DROP-OFF ZONES #plt.figure(figsize=(30,20)) dropoff_zone = train[&#39;DOLocationID&#39;] (unique, counts) = np.unique(dropoff_zone, return_counts=True) plt.bar(unique, counts, width=0.9, color=&#39;#8ef1df&#39;) plt.xlabel(&#39;Drop-off Zone ID&#39;) plt.ylabel(&#39;Number of trips&#39;) plt.suptitle(&quot;Number of trips by Drop-off Zone in Train Set&quot;, ha = &#39;right&#39;) plt.show() # Omnibus test of normality &amp; Shapiro-Wilk test of normality # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html # H0 := sample comes from normal dist from scipy.stats import normaltest, shapiro for col in NUM_COLS: print(&#39;&gt;&gt; Attribute:&#39;, col) p_train = normaltest(train[col])[1] p_test = normaltest(test[col])[1] if p_train&lt;0.05: print(&#39;The train set is NOT approximately normal&#39;) if p_train&gt;=0.05: print(&#39;The train set is approximately normal&#39;) if p_test&lt;0.05: print(&#39;The test set is NOT approximately normal&#39;) if p_test&gt;=0.05: print(&#39;The test set is approximately normal&#39;) print(&#39;-------------------------------&#39;) "],
["2-8-visualisation-python.html", "2.8 Visualisation (Python)", " 2.8 Visualisation (Python) 2.8.1 Sankey diagram import plotly.graph_objects as go df = pd.DataFrame({ &#39;PULocationID&#39;:np.load(&#39;D:/DS/0_ASS1/dataset/PULocationID_credit.npz&#39;)[&#39;data&#39;], &#39;DOLocationID&#39;:np.load(&#39;D:/DS/0_ASS1/dataset/DOLocationID_credit.npz&#39;)[&#39;data&#39;], &#39;total_amount&#39;:np.load(&#39;D:/DS/0_ASS1/dataset/total_amount_credit.npz&#39;)[&#39;data&#39;], &#39;pickup_hour&#39;:np.load(&#39;D:/DS/0_ASS1/dataset/pickup_hour_credit.npz&#39;)[&#39;data&#39;], }) df[&#39;PULocationID&#39;] = df[&#39;PULocationID&#39;].astype(int) df[&#39;DOLocationID&#39;] = df[&#39;DOLocationID&#39;].astype(int) df[&#39;pickup_hour&#39;] = df[&#39;pickup_hour&#39;].astype(int) df = df.sample(10000, random_state=26) # Create area mapping mapping = manhattan.to_dict(&#39;split&#39;)[&#39;data&#39;] mapping = {k:v for [k,v] in mapping} df[&#39;PUZone&#39;]=df[&#39;PULocationID&#39;].map(mapping).fillna(&#39;Other&#39;) df[&#39;DOZone&#39;]=df[&#39;DOLocationID&#39;].map(mapping).fillna(&#39;Other&#39;) def hex_to_rgb(hex_color, opacity): hex_color = hex_color.lstrip(&quot;#&quot;) if len(hex_color) == 3: hex_color = hex_color * 2 return int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16), opacity area_mapping = {&#39;Downtown&#39;:0, &#39;Midtown&#39;:1, &#39;Uptown&#39;:2, &#39;JFK&#39;:3, &#39;LGA&#39;:4, &#39;Other&#39;:5} color_mapping = {0:&#39;1380A1&#39;, 1:&#39;588300&#39;, 2:&#39;1ce4c1&#39;, 3:&#39;FAAB18&#39;, 4:&#39;990000&#39;, 5:&#39;bf55fb&#39;} data = df.groupby([&#39;PUZone&#39;,&#39;DOZone&#39;]).count().reset_index().iloc[:,:3] data[&#39;PUZone&#39;] = data[&#39;PUZone&#39;].map(area_mapping) data[&#39;DOZone&#39;] = data[&#39;DOZone&#39;].map(area_mapping) data[&#39;Color&#39;] = data[&#39;PUZone&#39;].map(color_mapping).apply(lambda x: &#39;rgba&#39;+str(hex_to_rgb(x,0.5))) data.head() # Plot fig = go.Figure(data=[go.Sankey( node = dict( pad = 15, thickness = 20, line = dict(width = 0), label = list(area_mapping.keys()), color = [&#39;#&#39;+str(x) for x in color_mapping.values()] ), link = dict( source = data[&#39;PUZone&#39;], # indices correspond to labels, eg A1, A2, A2, B1, ... target = data[&#39;DOZone&#39;], value = data[&#39;PULocationID&#39;], color = data[&#39;Color&#39;] ))]) fig.show() 2.8.2 Profitability fig, ax = plt.subplots(1, 1, figsize=(20,20)) cax = make_axes_locatable(ax).append_axes(&quot;bottom&quot;, size=&quot;1%&quot;, pad=0.1) ax = Zones.plot(alpha=0.9, ax=ax, edgecolor=None, column=&#39;log_profitability&#39;, cmap=Matter_20.mpl_colormap, cax = cax, legend=True, legend_kwds={&#39;orientation&#39;: &quot;horizontal&quot;}) ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron) ax.set_title(&#39;Average log profitability ($) per taxi zone in 2019&#39;, fontsize=24) ax.set_axis_off() 2.8.3 Airport stripe from palettable.cmocean.sequential import Matter_19 from sklearn.preprocessing import minmax_scale lga_pu[&#39;num_of_trips&#39;] = 1 agg = lga_pu.groupby([&#39;DOLocationID&#39;,&#39;pickup_hour&#39;]).sum().reset_index() ########## CREATING AGGREGATED DF to_append = {&#39;DOLocationID&#39;:[], &#39;pickup_hour&#39;:[], &#39;num_of_trips&#39;:[]} for i in agg[&#39;DOLocationID&#39;].unique(): avail = agg[agg[&#39;DOLocationID&#39;]==i][&#39;pickup_hour&#39;].unique() if len(avail)!=24: for j in range(24): if j not in avail: to_append[&#39;DOLocationID&#39;].append(i) to_append[&#39;pickup_hour&#39;].append(j) to_append[&#39;num_of_trips&#39;].append(0) agg = pd.concat([agg, pd.DataFrame(to_append)],ignore_index=True) agg = agg.sort_values(by=[&#39;DOLocationID&#39;,&#39;pickup_hour&#39;]).reset_index(drop=True) ######## CREATING HEATMAP heatmap = np.ndarray((len(agg[&#39;DOLocationID&#39;].unique()), len(agg[&#39;pickup_hour&#39;].unique()))) for n in range(agg.shape[0]): row = agg.iloc[n,:] i = n//24 heatmap[i][row[&#39;pickup_hour&#39;].astype(int)]=row[&#39;num_of_trips&#39;] heatmap=minmax_scale(heatmap) ####### PLOT fig = plt.figure(figsize=(60,20)) ax = fig.add_subplot(111) plt.imshow(heatmap.T,cmap=Matter_19.mpl_colormap) ax.set_aspect(&#39;equal&#39;) # We want to show all ticks... ax.set_xticks(np.arange(len(agg[&#39;DOLocationID&#39;].unique()))) ax.set_yticks(np.arange(len(agg[&#39;pickup_hour&#39;].unique()))) # ... and label them with the respective list entries ax.set_xticklabels(agg[&#39;DOLocationID&#39;].astype(int).unique()) ax.set_yticklabels(agg[&#39;pickup_hour&#39;].astype(int).unique()) ax.grid(False) # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=90, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) plt.show() "],
["2-9-linear-modelling-r.html", "2.9 Linear Modelling (R)", " 2.9 Linear Modelling (R) library(feather) library(psych) library(plotly) library(dplyr) library(tidyr) library(lme4) library(infotheo) # PALETTE PALETTE &lt;- c(&#39;#1380A1&#39;, &#39;#FAAB18&#39;, &#39;#990000&#39;, &#39;#588300&#39;, &#39;#bf55fb&#39;, &#39;#1ce4c1&#39;) FONT &lt;- list(family=&#39;Helvetica&#39;) # Load DS train &lt;- read_feather(&#39;train_set&#39;) test &lt;- read_feather(&#39;test_set&#39;) manhattan &lt;- read.csv(&#39;manhattan.csv&#39;) TAT &lt;- read.csv(&#39;Loc_Hour_TAT.csv&#39;) head(train) train &lt;- train %&gt;% left_join(manhattan, by=c(&#39;PULocationID&#39;=&#39;LocationID&#39;)) %&gt;% replace_na(list(&#39;Area&#39;=&#39;Other&#39;)) %&gt;% rename(PUArea=Area) %&gt;% left_join(manhattan, by=c(&#39;DOLocationID&#39;=&#39;LocationID&#39;)) %&gt;% replace_na(list(&#39;Area&#39;=&#39;Other&#39;)) %&gt;% rename(DOArea=Area) %&gt;% filter(total_amount &gt; 0) %&gt;% mutate(PUArea = as.factor(PUArea), DOArea = as.factor(DOArea), DOLocationID = as.factor(DOLocationID), PULocationID = as.factor(PULocationID)) %&gt;% left_join(TAT %&gt;% mutate(LocationID=as.factor(LocationID)), by=c(&#39;PULocationID&#39;=&#39;LocationID&#39;, &#39;pickup_hour&#39;=&#39;Hour&#39;)) %&gt;% drop_na() test &lt;- test %&gt;% left_join(manhattan, by=c(&#39;PULocationID&#39;=&#39;LocationID&#39;)) %&gt;% replace_na(list(&#39;Area&#39;=&#39;Other&#39;)) %&gt;% rename(PUArea=Area) %&gt;% left_join(manhattan, by=c(&#39;DOLocationID&#39;=&#39;LocationID&#39;)) %&gt;% replace_na(list(&#39;Area&#39;=&#39;Other&#39;)) %&gt;% rename(DOArea=Area) %&gt;% filter(total_amount &gt; 0) %&gt;% mutate(PUArea = as.factor(PUArea), DOArea = as.factor(DOArea), DOLocationID = as.factor(DOLocationID), PULocationID = as.factor(PULocationID)) %&gt;% left_join(TAT %&gt;% mutate(LocationID=as.factor(LocationID)), by=c(&#39;PULocationID&#39;=&#39;LocationID&#39;, &#39;pickup_hour&#39;=&#39;Hour&#39;)) %&gt;% drop_na() # Outlier analysis tip_rate &lt;- train$tip_amount-train$fare_amount upperbound &lt;- quantile(tip_rate[tip_rate&gt;0], 0.75)+1.5*quantile(tip_rate[tip_rate&gt;0], 0.75)-1.5*quantile(tip_rate[tip_rate&gt;0], 0.25) outliers &lt;- train[train$tip_amount-train$fare_amount&gt;=upperbound,] outliers &lt;- rbind(outliers, train[train$total_amount&gt;2.23,]) write.csv(outliers, &#39;outliers.csv&#39;) train &lt;- train %&gt;% anti_join(outliers) # Plot pairwise for 100 random points set.seed(26) sample &lt;- train %&gt;% sample_n(100) pairs.panels(sample, bg=PALETTE[sample$PUArea], pch=21, method = &quot;pearson&quot;, # correlation method hist.col = &quot;#FAAB18&quot;, density = FALSE, # show density plot, ellipses=FALSE, rug = FALSE, stars = TRUE, cex.cor = 2.5 ) # Comparing levels of locations bins &lt;- cut(train$total_amount, 100000) nmi &lt;- function(x,y) { return(2*mutinformation(x,y)/(entropy(x)+entropy(y))) } nmi(bins, train$PULocationID) nmi(bins, train$PUArea) nmi(bins, train$DOLocationID) nmi(bins, train$DOArea) # Test effect of PU Area on Total Amount # Non-parametric (median-based) ANOVA kruskal.test(total_amount ~ PUArea, data=sample) # Plot boxplot to compare data plot_ly(data=train, x=~PUArea, y=~total_amount, type=&#39;box&#39;, color=~Area, colors=PALETTE) %&gt;% layout(font=FONT, yaxis=list(title=&#39;Total Amount&#39;)) # Freq plot (Not used) ttable &lt;- as.data.frame.matrix(table(train %&gt;% select(Area, pickup_hour))) ttable$Area &lt;- row.names(ttable) row.names(ttable) &lt;- NULL ttable &lt;- ttable %&gt;% gather(key=&#39;Hour&#39;, value=&#39;Freq&#39;, `0`:`23`) ttable$Hour &lt;- as.numeric(ttable$Hour) plot_ly( data=ttable, x=~Hour, y=~Freq, color=~Area, name=~Area, colors=PALETTE, type=&#39;bar&#39;) %&gt;% layout(barmode=&#39;stack&#39;, bargap=0.1, xaxis=list(type=&#39;linear&#39;), yaxis=list(title=&#39;Total number of trips&#39;), font=FONT) # Proportion plot (Not used) ptable &lt;- prop.table(table(train %&gt;% select(Area, pickup_hour)),2) ptable &lt;- as.data.frame.matrix(ptable) ptable$Area &lt;- row.names(ptable) row.names(ptable) &lt;- NULL ptable &lt;- ptable %&gt;% gather(key=&#39;Hour&#39;, value=&#39;Prop&#39;, `0`:`23`) ptable$Hour &lt;- as.numeric(ptable$Hour) ptable$Prop &lt;- ptable$Prop*100 plot_ly( data=ptable, x=~Hour, y=~Prop, color=~Area, name=~Area, colors=PALETTE, type=&#39;bar&#39;) %&gt;% layout(barmode=&#39;stack&#39;, bargap=0.1, xaxis=list(type=&#39;linear&#39;), yaxis=list(title=&#39;Proportion (%)&#39;), font=FONT) # Simple linear model for sample 500 random points set.seed(26) sample &lt;- train %&gt;% sample_n(10000) # Forward feature selection &gt;&gt; Remove &quot;posteriori&quot; features aka dist, fare, tip m0.sample &lt;- lm(total_amount ~ (trip_duration+TAT+PUArea+DOArea)^2, data=sample) m0.sample &lt;- step(m0.sample, scope = ~., data=sample) anova(m0.sample) m0.sample &lt;- lm(total_amount ~ TAT+(trip_duration+PUArea+DOArea)^2, data=sample) anova(m0.sample) # Build models m1.sample &lt;- lm(total_amount ~ trip_duration, data=sample) m2.sample &lt;- lm(total_amount ~ trip_duration + PUArea + DOArea, data=sample) m3.sample &lt;- lm(total_amount ~ trip_duration + PUArea + DOArea + TAT, data=sample) m4.sample &lt;- lm(total_amount ~ trip_duration + PUArea * DOArea, data=sample) m5.sample &lt;- lm(total_amount ~ trip_duration * (PUArea + DOArea), data=sample) m6.sample &lt;- lm(total_amount ~ (trip_duration + PUArea + DOArea)^2, data=sample) m7.sample &lt;- lmer(total_amount ~ trip_duration + PUArea + DOArea + (1|PUArea) + (1|DOArea), data=sample) m8.sample &lt;- lmer(total_amount ~ trip_duration + PUArea + DOArea + (trip_duration|PUArea) + (trip_duration|DOArea), data=sample) m9.sample &lt;- lmer(total_amount ~ trip_duration + PUArea + DOArea + (TAT|PUArea) + (TAT|DOArea), data=sample, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) # Model comparison model_comparison &lt;- AIC(m0.sample, m1.sample, m2.sample, m3.sample, m4.sample, m5.sample, m6.sample, m7.sample, m8.sample, m9.sample) rmse &lt;- c() score &lt;- function(model) { 10^(sqrt(sum((predict(model, test) - test$total_amount)^2)/dim(test)[1])) } rmse &lt;- c(rmse, score(m0.sample)) rmse &lt;- c(rmse, score(m1.sample)) rmse &lt;- c(rmse, score(m2.sample)) rmse &lt;- c(rmse, score(m3.sample)) rmse &lt;- c(rmse, score(m4.sample)) rmse &lt;- c(rmse, score(m5.sample)) rmse &lt;- c(rmse, score(m6.sample)) rmse &lt;- c(rmse, score(m7.sample)) rmse &lt;- c(rmse, score(m8.sample)) rmse &lt;- c(rmse, score(m9.sample)) model_comparison$RMSE &lt;- rmse model_comparison$MLR &lt;- exp(1/2*(model_comparison$AIC[1]-model_comparison$AIC)) # Model comparison using AIC &amp; rmse plot model_comparison %&gt;% plot_ly( type=&#39;bar&#39;, y=~RMSE, color=~RMSE, colors=&#39;viridis&#39;) %&gt;% layout(font=FONT, yaxis=list(range=c(1.18,1.5)), xaxis=list(title=&#39;Model&#39;, dtick=1)) # Model comparison using AIC &amp; rmse model_comparison %&gt;% plot_ly( type=&#39;bar&#39;, y=~AIC, color=~AIC, colors=&#39;viridis&#39;) %&gt;% layout(font=FONT, yaxis=list(range=c(-25500,-18500)), xaxis=list(title=&#39;Model&#39;, dtick=1)) write.csv(model_comparison, &#39;model_comparison.csv&#39;) # Diagnostics par(mfrow = c(2, 2)) plot(m1.sample) par(mfrow = c(2, 2)) plot(m0.sample) # Plot predictions sample_visual &lt;- list() for (area in unique(test$PUArea)) { set.seed(7) sample_visual[[area]] &lt;- test %&gt;% filter(PUArea==area) %&gt;% sample_n(20) %&gt;% select(trip_duration, total_amount, PUArea, DOArea) %&gt;% distinct(trip_duration, PUArea, DOArea, .keep_all = TRUE) %&gt;% mutate(pred.m1=predict(m1.sample, .), pred.m4=predict(m2.sample, .), pred.m7=predict(m7.sample, .)) } sample_visual &lt;- do.call(rbind, sample_visual) %&gt;% arrange(trip_duration) sample_visual %&gt;% plot_ly() %&gt;% add_trace(x=~trip_duration, y=~total_amount, type=&#39;scatter&#39;, mode=&#39;markers&#39;, color=~PUArea, colors=PALETTE, alpha=0.5, marker=list(symbol=&#39;circle-open&#39;)) %&gt;% add_trace(x=~trip_duration, y=~pred.m1, type=&#39;scatter&#39;, mode=&#39;lines&#39;, name=&#39;Baseline&#39;, colors=&#39;black&#39;) %&gt;% add_trace(x=~trip_duration, y=~pred.m4, type=&#39;scatter&#39;, mode=&#39;lines&#39;, color=~PUArea, colors=PALETTE, line=list(dash=&#39;dot&#39;), alpha=0.5) %&gt;% add_trace(x=~trip_duration, y=~pred.m7, type=&#39;scatter&#39;, mode=&#39;lines&#39;, color=~PUArea, colors=PALETTE, line=list(dash=&#39;dash&#39;), alpha=0.5) %&gt;% layout(xaxis=list(type=&#39;linear&#39;, title=&#39;Trip duration&#39;), yaxis=list(type=&#39;linear&#39;, title=&#39;Total amount&#39;), font=FONT) # Fit m0 for the whole train set mfinal &lt;- lm(total_amount ~ TAT+(trip_duration+PUArea+DOArea)^2, data=train) summary(mfinal) predicted &lt;- test %&gt;% mutate(pred=predict(mfinal, .)) %&gt;% select(total_amount, pred, PUArea, DOArea) write.csv(summary(mfinal)$coef, &#39;finalModel.csv&#39;) remove(mfinal) # Free memory # Error analysis predicted %&gt;% #head(100) %&gt;% mutate(errors=10^(total_amount-pred), logerrors=total_amount-pred, Area=paste0(PUArea,&#39;:&#39;,DOArea)) %&gt;% plot_ly(x=~Area, y=~errors, type=&#39;box&#39;, marker=list(opacity=0), color=~Area, colors=PALETTE) %&gt;% layout(yaxis=list(type=&#39;linear&#39;, title=&#39;Error&#39;), xaxis=list(title=&#39;Area&#39;), font=FONT) "]
]
